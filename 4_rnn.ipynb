{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Jak przewidywać ceny na giełdzie?\n",
        "Użyjemy do tego RNN traktując wykres cen akcji jako szeregi czasowe.\n",
        "Posłużymy się najmocniejszym z poznanych jednostek rekurencyjnych - LSTM."
      ],
      "metadata": {
        "id": "sTt0CX0neZzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X8GhY2tvaBT"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Dense, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "gNb9EBOIvnOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ Część I ##############################\n",
        "# pobieramy dane (dzienne ceny akcji starbucks od lutego 2013 do lutego 2018)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/jgrynczewski/rnn/master/sbux.csv')"
      ],
      "metadata": {
        "id": "QYrZ0hah68gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "# Kiedy myślimy o cenie akcji mamy zazwyczaj w głowie jedną cenę/liczbę na dzień.\n",
        "# Więc moglibysmy myślec o tym jako o jednowymiarowym szerego czasowym.\n",
        "# Ale tu mamy wiele kolumn. Czym one są?\n",
        "\n",
        "# Zwrócmy uwagę na skalę. Ceny są podawane w dziesiątkach, podczas gdy wolumeny\n",
        "# w milionach"
      ],
      "metadata": {
        "id": "JRF82ptE8JzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "eBQ6Z4yi8TpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Widzimy, że przez 5 lat ceny akcji Starbucksa podwoiły się, co jest\n",
        "dobrą informacją dla wszystkich akcjonariuszy, posiadaczy akcji Starbucks"
      ],
      "metadata": {
        "id": "Zp2Xi0DrheLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zacznijmy z jednowymiarowym szeregiem czasowym. Skupmy się na cenie zamknięcia.\n",
        "\n",
        "series = df['close'].values.reshape(-1, 1)\n",
        "# Bierzemy wszystkie ceny zamknięca jako np array i zmianiamy wymiar na Nx1\n",
        "# Musimy to zrobić, ponieważ następny krok to zastosowanie StandardScalar z scikit-learn\n",
        "# po to, żeby dane były ustandaryzowane."
      ],
      "metadata": {
        "id": "XIXJVxWw8kzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizacja\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(series[:len(series)//2])  # fit robimy tylko na\n",
        "# pierwszej cześci szeregu (bo nie chcemy zanieczyścić danych testowych podczas treningu)\n",
        "series = scaler.transform(series).flatten()  # z kolei transform robimy na \n",
        "# całym zbiorze.\n",
        "# Dodatkowo, ponieważ będziemy okienkowali szereg musielibysmy zrobić kilka obliczeń,\n",
        "# żeby określić dokładnie granicę pomiędzy zbiorem treningowym i testowym. My sobie\n",
        "# uprościmy mówiąc, że ta granica jest dokładnie w połowie.\n",
        "# Na koniec spłaszczamy szereg, żeby dostać wektor długości N."
      ],
      "metadata": {
        "id": "2hR-iYV08xb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Okienkujemy nasz szereg.\n",
        "# Koncepcyjnie nie różni się to niczym od tego co robiliśmy z sinusem.\n",
        "# Jedyną różnicą są tu dane. Sposób postępowania jest niezmienny.\n",
        "T = 10\n",
        "D = 1\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for t in range(len(series) - T):\n",
        "  x = series[t:t+T]\n",
        "  X.append(x)\n",
        "  y = series[t+T]\n",
        "  Y.append(y)\n",
        "\n",
        "X = np.array(X).reshape(-1, T, 1)  # N x T x D\n",
        "Y = np.array(Y)\n",
        "N = len(X)\n",
        "\n",
        "print(f\"X.shape {X.shape}, Y.shape {Y.shape}\")"
      ],
      "metadata": {
        "id": "YuXiROoP9PTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Na sinusach poza LSTM testowaliśmy model autoregresyjny oraz SimpleRNN\n",
        "# Tutaj popatrzmy już na LSTM\n",
        "i = Input(shape=(T, 1))\n",
        "x = LSTM(5)(i)\n",
        "x = Dense(1)(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "    loss='mse',\n",
        "    optimizer=Adam(lr=0.1),\n",
        ")\n",
        "\n",
        "# train the RNN  - zbiór treningowy, pierwsza połowa. Zbiór testowy, druga połowa\n",
        "r = model.fit(\n",
        "    X[:-N//2],\n",
        "    Y[:-N//2],\n",
        "    epochs=80,\n",
        "    validation_data=(X[-N//2:], Y[-N//2:]),\n",
        ")"
      ],
      "metadata": {
        "id": "5qdoH1I49PXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# widzimy, że koszt spada co jest obiecujące\n",
        "# to oznacza, że LSTM jest w stanie przewidzieć coś blisko kolejnej próbki\n",
        "# szeregu czasowego.\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "J7zAA5kR-yvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jednokrokowa predykcja\n",
        "outputs = model.predict(X)\n",
        "print(outputs.shape)\n",
        "predictions = outputs[:, 0]\n",
        "\n",
        "plt.plot(Y, label='targets')\n",
        "plt.plot(predictions, label='predictions')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Wynik jest bardzo zgodny, ale dlaczego nie jest to dla nas \n",
        "# zbytnio pomocne. Bo tak naprawdę przewidujemy tylko cenę na kolejny\n",
        "# dzień, nie jest to informacja na podstawie której można podjąc\n",
        "# sensowne decyzje dotyczące sprzedaży/kupna akcji.\n",
        "# Chcielibyśmy przewidzieć dalekie tendencje, górki i dołki.\n",
        "# Może chociaż na dwa trzy dni do przodu. Popatrzmy"
      ],
      "metadata": {
        "id": "qhYPBCgg_F01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wielokrokowa predykcja\n",
        "validation_target = Y[-N//2:]\n",
        "validation_predictions = []\n",
        "\n",
        "# ostatni input \n",
        "last_x = X[-N//2] \n",
        "\n",
        "while len(validation_predictions) < len(validation_target):\n",
        "  p = model.predict(last_x.reshape(1, T, 1))[0, 0]  # 1x1 array -> scalar\n",
        "\n",
        "  validation_predictions.append(p)\n",
        "\n",
        "  last_x = np.roll(last_x, -1)\n",
        "  last_x[-1] = p\n",
        "\n",
        "plt.plot(validation_target, label='forecast target')\n",
        "plt.plot(validation_predictions, label='forecast prediction')\n",
        "plt.legend() # widzimy, że to już nie wygląda dobrze. Kiedy robimy prognoze wielokrokową\n",
        "# wszystko co dostajemy to prosta linia.\n",
        "\n",
        "# Czyli nasz model wcale nie jest taki świetny.\n",
        "# W zasadzie nie robi wiele. "
      ],
      "metadata": {
        "id": "9ScuG0DG_lIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ Część II ##############################\n",
        "# Lekcja 1 - jednokrokowa prognoza ceny jest myląca. Poza tym\n",
        "# współczesne modele w ogóle nie patrzą na cenę akcji.\n",
        "# To na co patrzą, to co próbują przewidzieć to zwrot akcji (stock return)\n",
        "# Zwrot definiujemy\n",
        "# R = V_final - V_initial / V_initial\n",
        "\n",
        "# To ta sama formuła, co wtedy kiedy przechodząc obok sklepu,\n",
        "# widzimy przecena. np. 20%. Co to oznacza ?\n",
        "# Powiedzmy, że coś kosztowało 100. Po przecenie kosztuje 80\n",
        "\n",
        "# 80 - 100 / 100 = -0.2 = -20% (czyli procentowy spadek/wzrost ceny)\n",
        "\n",
        "# Tak więc kiedy finansowi inżynierowie prognozują akcje, przeważnie\n",
        "# patrzą na pewną formę zwrotu, a nie na aktualną cenę.\n",
        "\n",
        "# Wrócmy do kodu.\n",
        "\n",
        "# Skoro pracujemy z Pandas ta zmiana będzie dość prosta. Wystarczy tylko wiedzieć\n",
        "# jakiej funkcji użyć.\n",
        "# Jak zwykle chcemy procesować nasze dane w postaci wektorów (tzn. wykonujemy\n",
        "# operacje na wszystkich kolumnach za jednym razem)\n",
        "\n",
        "# 1. Tworzymy nową kolumnę PrevClose taką, która jest przesuniętą o 1 ceną \n",
        "# zamknięcia poprzedniego dnia. Tak żeby wczorajsza cena zamknięcia (PrevClose)\n",
        "# była obok dziejszej ceny zamknięcia.\n",
        "df['PrevClose'] = df['close'].shift(1)\n",
        "\n",
        "# Teraz wygląda to tak\n",
        "# close / prev lose\n",
        "# x[2] x[1]\n",
        "# x[3] x[2]\n",
        "# x[4] x[3]\n",
        "# ...\n",
        "# x[t] x[t-1]"
      ],
      "metadata": {
        "id": "Pe4A_r2zAlLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # widzimy, że pierwsza wartość PrevClose to Nan,\n",
        "# ponieważ nie mamy ceny sprzed pierwszego dnia. Nie jest to\n",
        "# dla nas problem, ponieważ będziemy budować RNN, które bierze\n",
        "# pierwsze 10 dni, żeby przewidzieć 11. Czyli pierwszą wartość\n",
        "# którą będziemy przewidywać to wartość 11 (na podstawie \n",
        "# 10 poprzednich). Dopóki ten NaN nie jest celem, nie musimy się \n",
        "# tym przejmować."
      ],
      "metadata": {
        "id": "SHB5N3-9BoI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Liczymy zwroty - dodatkowa kolumna\n",
        "# x[t] - x[t-1] / x[t-1]\n",
        "df['Return'] = (df['close'] - df['PrevClose']) / df['PrevClose']"
      ],
      "metadata": {
        "id": "iqClW06cB3L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "gLqdBpIKCF78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Popatrzmy na rozkład zwrotów. Widzimy, że najczęściej jest to 0.\n",
        "# Widzimy, że zwroty są bardzo małymi wartościami. \n",
        "# Możemy chcieć je znormalizować. Zróbmy to.\n",
        "df['Return'].hist()"
      ],
      "metadata": {
        "id": "ko9DJutZCHt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series = df['Return'].values[1:].reshape(-1, 1)  # Po pierwsze, znowu robimy\n",
        "# z tego macierz Nx1, bo użyjemy StandardScaler.\n",
        "\n",
        "# Normalizacja zwrotów\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(series[:len(series) // 2])  # Nie normalizujemy danych testowych.\n",
        "# Normalizacja ma na celu jedynie zmniejszyć złożoność obliczeniową.\n",
        "# Na znormalizowanych danych obliczenia są szybsze.\n",
        "# fit na pierwszej połowie\n",
        "series = scaler.transform(series).flatten()  # transform na całym szeregu\n",
        "# i na koniec flatten do wektora długości N."
      ],
      "metadata": {
        "id": "VGuqJGWiCYs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### te same kroki, ale tym razem sekwencją nie są kolejne\n",
        "# ceny akcji, ale kolejne zwroty\n",
        "T = 10\n",
        "D = 1\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for t in range(len(series) - T):\n",
        "  x = series[t:t+T]\n",
        "  X.append(x)\n",
        "  y = series[t+T]\n",
        "  Y.append(y)\n",
        "\n",
        "X = np.array(X).reshape(-1, T, 1)  #  N x T x D\n",
        "Y = np.array(Y)\n",
        "N = len(X)\n",
        "print(f\"X.shape {X.shape}, Y.shape {Y.shape}\")"
      ],
      "metadata": {
        "id": "-0W308D6C20j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM, jedyną różnicą są tu dane. Kod ten sam.\n",
        "i = Input(shape=(T, 1))\n",
        "x = LSTM(5)(i)\n",
        "x = Dense(1)(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "    loss='mse',\n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "\n",
        "r = model.fit(\n",
        "    X[:-N//2],\n",
        "    Y[:-N//2],\n",
        "    epochs=80,\n",
        "    validation_data=(X[-N//2:], Y[-N//2:])\n",
        ")"
      ],
      "metadata": {
        "id": "0uxeILKWFe7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "\n",
        "# Co widzimy ?\n",
        "# Modelowi jest znacznie trudniej nauczyć się cokolwiek.\n",
        "# Koszt z każdą iteracją spada, ale na zbiorze walidacyjnym rośnie.\n",
        "# Innymi słowy, model coraz lepiej dopasowuje się do szumu."
      ],
      "metadata": {
        "id": "_3b9y7s7GGCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jednokrokowa predykcja\n",
        "outputs = model.predict(X)\n",
        "print(outputs.shape)\n",
        "predictions = outputs[:, 0]\n",
        "\n",
        "plt.plot(Y, label='targets')\n",
        "plt.plot(predictions, label='predictions')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Na podstawie tego wykresu trudno jest ocenić, czy prognozy są\n",
        "# poprawne, ale znając kolejne wartościfunkcji kosztu domyślamy się, że nie.\n",
        "# Można to uruchomić lokalnie i zoomować sprawdzając."
      ],
      "metadata": {
        "id": "66mCngL3Jh7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wielokrokowa predykcja\n",
        "validation_target = Y[-N//2:]\n",
        "validation_predictions = []\n",
        "\n",
        "last_x = X[-N//2] \n",
        "\n",
        "while len(validation_predictions) < len(validation_target):\n",
        "  p = model.predict(last_x.reshape(1, T, 1))[0, 0]  # 1x1 array -> scalar\n",
        "\n",
        "  validation_predictions.append(p)\n",
        "\n",
        "  last_x = np.roll(last_x, -1)\n",
        "  last_x[-1] = p\n",
        "\n",
        "plt.plot(validation_target, label='forecast target')\n",
        "plt.plot(validation_predictions, label='forecast prediction')\n",
        "plt.legend()\n",
        "\n",
        "# Znów mamy do czynienia z sytaucją kiedy model potrafi tylko kopiować\n",
        "# jedną wartość w kółko. "
      ],
      "metadata": {
        "id": "oryaGD9lJ5Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################### Część III #########################\n",
        "# Model 3 - Zbudujmy lepszy model\n",
        "# W modelu 2 zdecydowaliśmy się na regresję.\n",
        "# Popatrzmy na to zagadnienie z perspektywy klasyfikacji.\n",
        "# Użyjemy wszystkich informacji - cena otwarcia, najwyższ, najniższa, zamknięcia oraz wolumen.\n",
        "# A na podstawie tych informacji będziemy starali się przewidzieć czy cena pójdzie\n",
        "# w górę, czy w dół.\n",
        "\n",
        "# Klasyfikajca: w górę ?\n",
        "\n",
        "# Czyli będziemy mieli input TxD (gdzie T=10, D=5 - liczba cech)\n",
        "# No i na jego podstawie nie będziemy próbowali przewidzieć zwrotu, zamiast \n",
        "# tego zamienimy to w najprostsze możliwe zadanie. Spróbujemy przewidzieć, \n",
        "# czy cena pójdzie w górę, czy w dół - klasyfikajca binarna.\n",
        "\n",
        "# W ogólności w klasyfikacji łatwiej otrzymać satysfakcjonujące wyniki niż w regresji.\n",
        "# W regresji przewidujemy ciągłe wartości. Nie możesz być za wysoko czy za nisko,\n",
        "# musisz być dokładnie w tej wartości. Klasyfikacja jest prostsza. Zwłaszcza\n",
        "# klasyfikacja binarna. Nie trzeba przewidywać dokładnej wartości. Wystarczy\n",
        "# etykieta. W tym przypadku mamy tylko dwie wartości: do góry, do dołu.\n",
        "# I zazwyczaj to jest właśnie to co nas interesuje w przypadku akcji."
      ],
      "metadata": {
        "id": "rh0ZUeqnLI93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Załadujmy wszystkie dane do numpy arrays\n",
        "\n",
        "input_data = df[['open', 'high', 'low', 'close', 'volume']].values\n",
        "targets = df['Return'].values  # target będziemy określali na podstawie zwrotu."
      ],
      "metadata": {
        "id": "EAfsX6J6LNTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ustawmy N, T i D\n",
        "T = 10  # liczba elementów na podstawie których prognozujemy następną wartość\n",
        "D = input_data.shape[1]  # liczba kolumn danych wejściowych\n",
        "N = len(input_data) - T # N jest trochę podchwytliwe. Długość szeregu - okienko.\n",
        "# Liczyliśmy to kilka razy.\n",
        "# (na przykład jeżeli T=10 i mamy tylko 11 próbek wtedy mamy N=1)"
      ],
      "metadata": {
        "id": "oqaq_CnqLiue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizacja - tym razem jest to szczególnie istotne ponieważ\n",
        "# kolumna wolumenu ma bardzo duże liczby, w porównaniu do wartości w kolumnach\n",
        "# z ceną.\n",
        "# Tym razem zrobimy to trochę łatwiejsze dla naszego modelu (więcej treningu).\n",
        "# Zamiast ustawiać pierwszą połowę jako zbiór uczący, a drugą jako testowy,\n",
        "# zróbmy 2/3 zbiorem treningowym, a pozostałe 1/3 zbiorem testowym.\n",
        "Ntrain = len(input_data) * 2 // 3\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(input_data[:Ntrain + T])\n",
        "input_data = scaler.transform(input_data)"
      ],
      "metadata": {
        "id": "l6UklkuZL8U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzymy zbiór treningowy\n",
        "# X_train jest rozmiaru Ntrain x T x D\n",
        "# Y_train jest rozmiaru Ntrain\n",
        "X_train = np.zeros((Ntrain, T, D))\n",
        "Y_train = np.zeros(Ntrain)\n",
        "\n",
        "# Wypełniamy dane\n",
        "for t in range(Ntrain):\n",
        "  X_train[t, :, :] = input_data[t:t+T] # X to input_data od t do t+T\n",
        "  Y_train[t] = (targets[t+T] > 0)  # Y to informacja o tym, czy zwrot w t+T był dodatni. "
      ],
      "metadata": {
        "id": "qh9yiy5mMdD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzymy zbiór uczący\n",
        "# Ntest = N - Ntrain\n",
        "X_test = np.zeros((N - Ntrain, T, D))\n",
        "Y_test = np.zeros(N - Ntrain)\n",
        "\n",
        "for u in range(N - Ntrain):\n",
        "  # u of 0 do (N - Ntrain)\n",
        "  # t od Ntrain do N\n",
        "  t = u + Ntrain # indexujemy oryginalny zbiór danych, czyli musimy\n",
        "  # zachować offset Ntrain. Uzywamy t do indeksowania oryginalnego\n",
        "  # zbiou, a u do indeksowania X_test i Y_test.\n",
        "  X_test[u, :, :] = input_data[t:t+T]\n",
        "  Y_test[u] = (targets[t+T] > 0)"
      ],
      "metadata": {
        "id": "ZHzwbMvKNEYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pamiętamy, że tym razem robimy klasyfikację bianarną\n",
        "# więc ostatnia warstwa to ma jeden węzeł i funkcję aktywacji - sigmoid.\n",
        "# loss to binary_crossentropy\n",
        "i = Input(shape=(T, D))\n",
        "x = LSTM(50)(i)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "-mhhc21HNvcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trenujemy\n",
        "r = model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    batch_size=32,\n",
        "    epochs=300,\n",
        "    validation_data=(X_test, Y_test)\n",
        ")"
      ],
      "metadata": {
        "id": "MfXf_iiDOVed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# koszt\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Widzimy, że koszt zbioru treningowego spada lekko, ale koszt na zbiorze \n",
        "# walidującym mocno wzrasta. To mówim nam, że model znów przeucza się szumu. "
      ],
      "metadata": {
        "id": "wH3JN2ObOm48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jak popatrzymy na dokładność widzimy to samo.\n",
        "# Dokładnośc na zbiorze treningowym wzrasta, ale dokładnośc na zbiorze\n",
        "# walidacyjnym nie.\n",
        "# Można zadać sobie pytanie dlaczego dokładnośc na zbiorze testowym nie idzie w dół,\n",
        "# tylko pozostaje na 0.5. Dla klasyfikacji binarnej 0 nie jest wcale najgorszą \n",
        "# dokładnością. Jeżeli twoja dokładność wynosi 0 oznacza to, że wystarczy tylko \n",
        "# odwrócić prognozy i będziemy mieli 100 % dokładność.\n",
        "# Dla klasyfikacji bianrnej najgorszą dokładnością jest 0.5. 0.5 oznacza, że\n",
        "# model zachowuje się jakby zgadywał, wybierał losowo, rzucał monetą. Dlatego\n",
        "# kiedy model się przeucza dokładnośc modelu na zbiorze testowym pozostaje 0.5.\n",
        "\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ybD1kmbPO3sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I to tyle. Aktualnie nie istniej model, który potrafiłby poprawnie przewidywać ceny akcji. \n",
        "\n",
        "Zrobiliśmy trzy podejścia do tematu:\n",
        "1. próbowaliśmy przewidzieć przyszłą cenę akcji na podstawie przeszłej ceny za pomocą modelu LSTM ('zadziałało' tylko dla jednokrokowej prognozy). Model zachowywał się tak jakby pamiętał tylko poprzednią wartość i na jej podstawie prognozował.\n",
        "2. próbowaliśmy przewidzieć zwrot na podstawie poprzednich zwrotów. W tym przkładzie nie udało się uzyskać dużego spadku funkcji kosztu.\n",
        "3. użyliśmy wszystkich dostępnych danych i klasfikatora binarnego.\n",
        "\n",
        "Nawet próba rozwiązania zagadnienia w możliwe najprostszym sformułowaniu dała nie lepsze rezultaty niż losowe zgadywanie wyniku.\n",
        "\n",
        "Lekcja?\n",
        "Nie ma mowy, żeby dwa pierwsze modele zadziałały. Skoro nie potrafimy przewidzieć tego czy cena akcji pójdzie w górę czy w dół jak moglibyśmy przewidzieć numeryczną wartość zwroty czy ceny.\n",
        "\n",
        "Bądź podejrzliwy kiedy ktoś mówi o modelu dokładnie przewidującym ceny akcji.\n",
        "\n",
        "Podstawowym probleme w przewidywaniu cen akcji na podstawie cen akcji jest to, że nie uwzględniamy danych świata rzeczywistego, które mają wpływ na te ceny. Cena jest skorelowana z poprzednią wartością, ale znacznie silniej może być skorelowana z jakimś wydarzeniem na świecie. Nawet jeżeli zaczniemy uwzględniać różne wydarzenia napotkamy ogromne trudności związane z chaotycznym charakterem zjawiska.\n",
        "Poza tym na cenę akcji wpływ mają też takie rzeczy jak nastroje inwestorów czy wizerunek firmy w mediach. Nawet bez znajomości DL, powinno wzbudzać nasze podejrzenie informacje o kursach, metodach przewidywania cen akcji na podstawie danych historycznych.\n",
        "\n",
        "Stwierdzenie, że na podstawie danych historycznych ktoś jest w stanie przewidzieć ceny akcji jest równoznaczne ze stwierdzeniem, że na podstawie historycznych cen akcji ktoś jest w stanie przewidzieć dochodzenie dotyczące facebooka, wysłanie głupiego tweeta przez Elona Muska czy opracowanie nowej rakiety.\n",
        "\n",
        "Ale nie oznacza to, że LSTM nie są dobre.\n",
        "LSTM udowadniają swoją skuteczność w takich dziedzinach jak modelowanie języka i tłumaczenie maszynowe. Widzieliśmy już jak LSTM może być użyte do klasyfikacji zdjęć.\n",
        "\n",
        "Ważne, żeby umieć rozróżnić przewidywanie wartości na wiele kroków do przodu od przewidywania jednego kroku do przodu. Przewidywanie jeden krok do przodu nie jest błędem o ile stosuje się go w odpowiednim miejscu, a nie jako model przewidujący na wiele kroków do przodu.\n"
      ],
      "metadata": {
        "id": "SBb2WALbMsEJ"
      }
    }
  ]
}